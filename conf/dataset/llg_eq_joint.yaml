data:
  pde: 'llg'
  name: 'llg'
  datapath: 'data/llg.hdf5'  # path to save the generated dataset

start_at_t0: false        # whether to randomize initial time t0 for each sample
method: "joint"  # "joint" for learning forward and inverse, "forward" for forward only
residual_estimation: null # used for physics loss, either "ME" or "SE"
pretrained_path: null  # path to pretrained model for physics loss, null if training from scratch

net:
  in_ch: 6      # 2 * 3 for magnetization vector (m_x, m_y, m_z)
  label_ch: 4   # 3 for external field + 1 for time

training:
  batch_size: 32
  shuffle: True
  num_epochs: 1000
  learning_rate: 1e-4
  weight_decay: 0.0
  physics_loss: false       # apply physics loss during training
  physics_loss_coeff: 1.0   # coefficient for physics loss term
  gradient_clipping: 1.0    # max norm for gradient clipping
  val_interval: 10          # validate every n epochs
  checkpoint_interval: 20   # save checkpoint every n epochs
  ema_decay: 0.999          # exponential moving average decay
  ema_warmup: 20            # number of epochs to warm up EMA
  ema_update_interval: 1    # update EMA every n steps
  ema_device: "cpu"         # device for EMA parameters

sampling:
  sample_shape: [6, 64, 16] # shape of single sample (C, H, W)
  batch_size: 32            # batch size for sampling
  ch_a: 3
  zeta_a: 50.0
  zeta_u: 40.0
  zeta_pde: 0.4
  num_steps: 50
  loss_func: "diffusion_pde.sampling.heat_loss"