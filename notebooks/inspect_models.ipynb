{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85fb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import diffusion_pde as dpde\n",
    "from wandb.apis.public.runs import Runs\n",
    "from wandb.apis.public.artifacts import Artifacts, RunArtifacts, ArtifactFiles\n",
    "#from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44919a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphiliphohwy\u001b[0m (\u001b[33mphiliphohwy-danmarks-tekniske-universitet-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd51c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with initialize(config_path=\"../conf\", version_base=None):\n",
    "#    cfg = compose(\n",
    "#        config_name=\"train\", \n",
    "#        overrides=[\n",
    "#            \"dataset=heat_eq_logt_joint\",\n",
    "#            \"model=unetv2\"\n",
    "#        ]\n",
    "#    )\n",
    "\n",
    "wandb_cfg = OmegaConf.load(\"../conf/train.yaml\").wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d42c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {}\n",
    "\n",
    "runs = Runs(\n",
    "    client=api.client,\n",
    "    entity=wandb_cfg.entity,\n",
    "    project=wandb_cfg.project,\n",
    "    filters=filters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a50e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: hodjisac, Name: heat-logt/forward/unet-v2\n",
      "Run ID: 8nb62ytp, Name: heat-logt/forward/unet-v2\n",
      "Run ID: 4labp6a8, Name: heat-logt/joint/unet-v2\n",
      "Run ID: lykjcqiu, Name: heat-logt/joint/unet-v2\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    print(f\"Run ID: {run.id}, Name: {run.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1574c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Name: heat-logt-joint-unet-v2:v0, Type: model\n",
      "Artifact Name: run-4labp6a8-history:v0, Type: wandb-history\n"
     ]
    }
   ],
   "source": [
    "run_idx = 2\n",
    "run_cfg = OmegaConf.create(runs[run_idx].config)\n",
    "\n",
    "artifacts = RunArtifacts(\n",
    "    client=api.client,\n",
    "    run=runs[run_idx]\n",
    "    )\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"Artifact Name: {artifact.name}, Type: {artifact.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107628b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001\n",
      "model:\n",
      "  name: unet-v2\n",
      "  emb_ch: 256\n",
      "  obs_ch: 0\n",
      "  base_ch: 64\n",
      "  dropout: 0\n",
      "  ch_mults:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 2\n",
      "  noise_ch: 64\n",
      "  sigma_data: 0.5\n",
      "  n_res_blocks: 2\n",
      "wandb:\n",
      "  dir: /home/s204790/dynamical-pde-diffusion/logs\n",
      "  name: None\n",
      "  entity: philiphohwy-danmarks-tekniske-universitet-dtu\n",
      "  project: dynamical-pde-diffusion-final-final\n",
      "epochs: 1000\n",
      "dataset:\n",
      "  net:\n",
      "    in_ch: 2\n",
      "    label_ch: 2\n",
      "  data:\n",
      "    pde: heat\n",
      "    name: heat_logt\n",
      "    datapath: data/heat_logt.hdf5\n",
      "  loss: edm\n",
      "  method: joint\n",
      "  sampling:\n",
      "    ch_a: 1\n",
      "    zeta_a: 50\n",
      "    zeta_u: 40\n",
      "    zeta_pde: 0.4\n",
      "    loss_func: diffusion_pde.sampling.heat_loss\n",
      "    num_steps: 50\n",
      "    batch_size: 32\n",
      "    sample_shape:\n",
      "    - 2\n",
      "    - 64\n",
      "    - 64\n",
      "  training:\n",
      "    shuffle: true\n",
      "    batch_size: 64\n",
      "    num_epochs: 1000\n",
      "    physics_loss: false\n",
      "    weight_decay: 0\n",
      "    learning_rate: 0.0001\n",
      "    physics_loss_coeff: 1\n",
      "  start_at_t0: true\n",
      "run_name: heat-logt/joint/unet-v2\n",
      "weight_decay: 0\n",
      "model_save_path: /home/s204790/dynamical-pde-diffusion/pretrained_models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(run_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44cc2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "model_path = artifacts[0].download(root=f\"../pretrained_models/{runs[run_idx].id}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f111bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pretrained_models/4labp6a8/\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2e2a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edm = dpde.utils.get_net_from_config(run_cfg)\n",
    "edm.load_state_dict(torch.load(f\"{model_path}/model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce3b1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edm_sampler2(\n",
    "    net,            # EDMWrapper (calls Unet inside)\n",
    "    device,         # device to run the sampler on  \n",
    "    sample_shape,   # (B, C, H, W) shape of samples\n",
    "    loss_fn,        # loss function to compute gradients\n",
    "    loss_fn_kwargs, # extra args to pass to loss function\n",
    "    labels,         # (B, label_dim) extra conditioning your Unet expects; use zeros if None\n",
    "    zeta_a=1.0,     # weight for obs_a loss\n",
    "    zeta_u=1.0,     # weight for obs_u loss\n",
    "    zeta_pde=1.0,   # weight for pde loss\n",
    "    num_steps=18,\n",
    "    sigma_min=0.002,\n",
    "    sigma_max=80.0,\n",
    "    rho=7.0,\n",
    "    to_cpu=True,\n",
    "    generator=None,\n",
    "    return_losses=False,\n",
    "):\n",
    "    dtype_f = torch.float32     # net runs in fp32\n",
    "    dtype_t = torch.float64     # keep time grid in fp64 for stability, as in EDM\n",
    "\n",
    "    B = sample_shape[0]\n",
    "    \n",
    "    net.to(device=device)\n",
    "        \n",
    "    labels = labels.to(device=device, dtype=dtype_f)    # move labels to correct device and dtype\n",
    "\n",
    "    if generator is None:\n",
    "        generator = torch.Generator(device=device)\n",
    "\n",
    "    # Initial sample at sigma_max\n",
    "    latents = torch.randn(sample_shape, device=device, generator=generator)\n",
    "    \n",
    "    # Move loss function kwargs to correct device and dtype\n",
    "    loss_kwargs = loss_fn_kwargs.copy()\n",
    "    for key, val in loss_fn_kwargs.items():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            loss_kwargs[key] = val.clone().to(device=device, dtype=dtype_t)\n",
    "\n",
    "    # Discretize sigmas per EDM (Karras et al. 2022), t_N = 0 appended.\n",
    "    step_idx = torch.arange(num_steps, dtype=dtype_t, device=device)\n",
    "    sigmas = (sigma_max**(1.0/rho) + step_idx/(num_steps-1) * (sigma_min**(1.0/rho) - sigma_max**(1.0/rho)))**rho\n",
    "    sigmas = getattr(net, \"round_sigma\", lambda x: x)(sigmas)\n",
    "    sigmas = torch.cat([sigmas, torch.zeros_like(sigmas[:1])])  # length N+1, last = 0\n",
    "\n",
    "    # Initialize x at sigma_0\n",
    "    x_next = (latents.to(dtype_t) * sigmas[0])\n",
    "\n",
    "    losses = torch.zeros((num_steps, 4))  # for debugging\n",
    "    \n",
    "    for i, (sigma_cur, sigma_next) in enumerate(zip(sigmas[:-1], sigmas[1:])):  # i = 0..N-1\n",
    "        x_cur = x_next.detach().clone()\n",
    "        x_cur.requires_grad = True\n",
    "        # Euler step to t_next\n",
    "        x_N, dxdt = dpde.sampling.X_and_dXdt_fd(net, x_cur.to(dtype_f), torch.full((B,), sigma_cur, device=device, dtype=dtype_f), labels)\n",
    "        x_N, dxdt = x_N.to(dtype_t), dxdt.to(dtype_t)\n",
    "        \n",
    "        d_cur = (x_cur - x_N) / sigma_cur\n",
    "        x_next = x_cur + (sigma_next - sigma_cur) * d_cur\n",
    "        # Heun (2nd-order) correction unless final step\n",
    "        if i < num_steps - 1:\n",
    "            x_N, dxdt = dpde.sampling.X_and_dXdt_fd(net, x_next.to(dtype_f), torch.full((B,), sigma_next, device=device, dtype=dtype_f), labels)\n",
    "            x_N, dxdt = x_N.to(dtype_t), dxdt.to(dtype_t)\n",
    "            d_prime = (x_next - x_N) / sigma_next\n",
    "            x_next = x_cur + (sigma_next - sigma_cur) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "        # Compute losses\n",
    "\n",
    "        loss_pde, loss_a, loss_obs_u = loss_fn(x_N, dxdt, **loss_kwargs)\n",
    "        \n",
    "        if i <= 0.8 * num_steps:\n",
    "            w_a, w_u, w_pde = zeta_a, zeta_u, zeta_pde\n",
    "        else:\n",
    "            w_a, w_u, w_pde = 0.1 * zeta_a, 0.1 * zeta_u, zeta_pde\n",
    "        \n",
    "        loss_comb = w_a * loss_a + w_u * loss_obs_u + w_pde * loss_pde\n",
    "        grad_x = torch.autograd.grad(loss_comb, x_cur, retain_graph=False)[0]\n",
    "        x_next = x_next - grad_x\n",
    "        \n",
    "        losses[i] = torch.tensor([loss_a.item(), loss_obs_u.item(), loss_pde.item(), loss_comb.item()])\n",
    "\n",
    "        del x_cur, x_N, dxdt, d_cur, loss_pde, loss_a, loss_obs_u, loss_comb, grad_x\n",
    "    \n",
    "\n",
    "    # Return at sigma=0 in fp32\n",
    "    x = x_next.to(dtype_f).detach()\n",
    "    if to_cpu:\n",
    "        x = x.cpu()\n",
    "\n",
    "    losses = losses.detach().cpu().numpy() if return_losses else None\n",
    "\n",
    "    net.to(device=torch.device(\"cpu\"))\n",
    "    \n",
    "    del x_next, loss_kwargs\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return x, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_a = torch.tensor(A)\n",
    "obs_u = torch.tensor(U)\n",
    "\n",
    "dx = 1. / (A.shape[-1]-1)\n",
    "ch_a = 1\n",
    "\n",
    "sample_shape = (16, 2, 64, 64)\n",
    "\n",
    "zeta_a = 30.0\n",
    "zeta_u = 10.0\n",
    "zeta_pde = 0.5\n",
    "\n",
    "num_steps = 40\n",
    "\n",
    "t_cond = torch.ones(sample_shape[0]) * T\n",
    "alpha_cond = torch.ones_like(t_cond) * alpha\n",
    "labels = torch.stack([t_cond, alpha_cond], dim=-1)\n",
    "\n",
    "loss_fn_kwargs = {\n",
    "    \"obs_a\": obs_a,\n",
    "    \"obs_u\": obs_u,\n",
    "    \"mask_a\": mask_a,\n",
    "    \"mask_u\": mask_u,\n",
    "    \"dx\": dx,\n",
    "    \"dy\": dx,\n",
    "    \"ch_a\": ch_a,\n",
    "    \"labels\": alpha_cond,\n",
    "}\n",
    "\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "\n",
    "#for i in range(10):\n",
    "samples, losses = edm_sampler2(\n",
    "    net=edm,\n",
    "    device=device,\n",
    "    sample_shape=sample_shape,\n",
    "    loss_fn=dpde.sampling.heat_loss,\n",
    "    loss_fn_kwargs=loss_fn_kwargs,\n",
    "    labels=labels,\n",
    "    zeta_a=zeta_a,\n",
    "    zeta_u=zeta_u,\n",
    "    zeta_pde=zeta_pde,\n",
    "    num_steps=num_steps,\n",
    "    to_cpu=True,\n",
    "    return_losses=True,\n",
    ")\n",
    "print(f\"Final total loss: {losses[-1, 3]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyndiffenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
