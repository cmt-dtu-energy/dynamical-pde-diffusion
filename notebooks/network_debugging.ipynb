{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf7eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import diffusion_pde as dpde\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import initialize_config_dir, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a5412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module, mode=\"kaiming_normal\", zero_bias=True, nonlinearity=\"linear\"):\n",
    "    if not isinstance(module, (torch.nn.Conv2d, torch.nn.ConvTranspose2d, torch.nn.Linear)):\n",
    "        return\n",
    "    if mode == \"kaiming_normal\":\n",
    "        torch.nn.init.kaiming_normal_(module.weight, a=0, mode=\"fan_in\", nonlinearity=nonlinearity)\n",
    "        if zero_bias and module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "        elif module.bias is not None:\n",
    "            torch.nn.init.kaiming_normal_(module.bias.unsqueeze(1), a=0, mode=\"fan_in\", nonlinearity=nonlinearity)\n",
    "    elif mode == \"kaiming_uniform\":\n",
    "        torch.nn.init.kaiming_uniform_(module.weight, a=0, mode=\"fan_in\", nonlinearity=nonlinearity)\n",
    "        if zero_bias and module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "        elif module.bias is not None:\n",
    "            torch.nn.init.kaiming_uniform_(module.bias.unsqueeze(1), a=0, mode=\"fan_in\", nonlinearity=nonlinearity)\n",
    "    elif mode == \"zeros\":\n",
    "        torch.nn.init.zeros_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization mode: {mode}\")\n",
    "\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.max_positions = max_positions\n",
    "        self.endpoint = endpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n",
    "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
    "        freqs = (1 / self.max_positions) ** freqs\n",
    "        x = x.ger(freqs.to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "class Unet(torch.nn.Module):\n",
    "    '''\n",
    "    Unet taken from deep learning course.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chs: list[int], # list of channels including input channel size: (ch_in, ch_1, ..., ch_n), length n+1\n",
    "        label_ch: int, # label dimension (class label/ time etc)\n",
    "        noise_ch: int = 32, # embedding channel size \n",
    "        act_fn: torch.nn.Module = torch.nn.SiLU,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act_fn = act_fn\n",
    "        self.debug = debug\n",
    "\n",
    "        self.down_conv = torch.nn.ModuleList()\n",
    "        self.up_conv = torch.nn.ModuleList()\n",
    "\n",
    "        # Construct down-sampling blocks\n",
    "        for i in range(len(chs) - 1):\n",
    "            block = torch.nn.ModuleList()\n",
    "            if i != 0:\n",
    "                block.append(\n",
    "                    torch.nn.MaxPool2d(kernel_size=2, stride=2)               \n",
    "                )\n",
    "            block.extend(\n",
    "                (torch.nn.Conv2d(chs[i], chs[i+1], kernel_size=3, padding=1),\n",
    "                self.act_fn())\n",
    "            )\n",
    "            self.down_conv.append(torch.nn.Sequential(*block))\n",
    "\n",
    "        # Construct up-sampling blocks\n",
    "        for i in range(len(chs) - 1, 0, -1):\n",
    "            block = torch.nn.ModuleList()\n",
    "            if i == len(chs) - 1:\n",
    "                layer = torch.nn.ConvTranspose2d(chs[i], chs[i-1], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "            elif i == 1:\n",
    "                layer = torch.nn.ConvTranspose2d(chs[i] * 2, chs[i], kernel_size=3, padding=1)  \n",
    "            else:\n",
    "                layer = torch.nn.ConvTranspose2d(chs[i] * 2, chs[i-1], kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "            block.extend((layer, self.act_fn()))\n",
    "            if i == 1:\n",
    "                block.append(torch.nn.Conv2d(chs[i], chs[i-1], kernel_size=3, padding=1))\n",
    "            self.up_conv.append(torch.nn.Sequential(*block))\n",
    "\n",
    "\n",
    "        self.sigma_embedding = PositionalEmbedding(noise_ch)\n",
    "\n",
    "        if label_ch > 0:\n",
    "            self.linear_label = torch.nn.Linear(label_ch, noise_ch)\n",
    "\n",
    "        self.linear_embed = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(noise_ch, chs[i]) for i in range(1, len(chs), 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, sigma, labels=None):\n",
    "        #assume x has shape (b, ch, h, w) and t has shape (b, label_ch)\n",
    "\n",
    "        emb = self.sigma_embedding(sigma)\n",
    "        if labels is not None:\n",
    "            label_emb = self.linear_label(labels)\n",
    "            emb = emb + label_emb\n",
    "        embs = [self.linear_embed[i](emb) for i in range(len(self.linear_embed))]\n",
    "\n",
    "        signal = x\n",
    "        signals = []\n",
    "        for i, conv in enumerate(self.down_conv):\n",
    "            signal = conv(signal)\n",
    "            signal = signal  + embs[i][..., None, None]\n",
    "            if i < len(self.down_conv) - 1:\n",
    "                signals.append(signal)\n",
    "\n",
    "            if self.debug: print(f\"Down conv {i}: {signal.shape}\")\n",
    "\n",
    "        for i, tconv in enumerate(self.up_conv):\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "            if i < len(self.up_conv) - 1:\n",
    "                signal = signal + embs[-i-2][..., None, None]\n",
    "                \n",
    "            if self.debug: print(f\"Up conv {i}: {signal.shape}\")\n",
    "        return signal\n",
    "    \n",
    "\n",
    "def get_conv_layer(\n",
    "    in_ch: int, \n",
    "    out_ch: int, \n",
    "    kernel_size: int, \n",
    "    up: bool = False,\n",
    "    down: bool = False,\n",
    "    init_mode: str = \"kaiming_normal\",\n",
    "):\n",
    "\n",
    "    padding = max(0, (kernel_size - 1) // 2)\n",
    "    if up:\n",
    "        layer = torch.nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride=2, padding=padding, output_padding=1, padding_mode=\"zeros\")\n",
    "    elif down:\n",
    "        layer = torch.nn.Conv2d(in_ch, out_ch, kernel_size, stride=2, padding=padding, padding_mode=\"reflect\")\n",
    "    else:\n",
    "        layer = torch.nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding, padding_mode=\"reflect\")\n",
    "    init_weights(layer, mode=init_mode)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        out_ch: int,\n",
    "        emb_ch: int,\n",
    "        up: bool = False,\n",
    "        down: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        skip_scale: float = 2 ** -0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_ch\n",
    "        self.out_channels = out_ch\n",
    "        self.skip_scale = skip_scale\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "\n",
    "        # Norms\n",
    "        gn1_groups = 32 if in_ch >= 32 and in_ch % 32 == 0 else in_ch\n",
    "        gn2_groups = 32 if out_ch >= 32 and out_ch % 32 == 0 else out_ch\n",
    "        self.norm1 = torch.nn.GroupNorm(gn1_groups, in_ch)\n",
    "        self.norm2 = torch.nn.GroupNorm(gn2_groups, out_ch)\n",
    "\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "        # Convs\n",
    "        self.conv1 = get_conv_layer(in_ch, out_ch, 3, up=up, down=down)\n",
    "        self.conv2 = get_conv_layer(out_ch, out_ch, 3, init_mode=\"zeros\")\n",
    "\n",
    "        # Embedding â†’ out_ch\n",
    "        self.emb_layer = torch.nn.Linear(emb_ch, out_ch)\n",
    "        init_weights(self.emb_layer)\n",
    "\n",
    "        # Skip path\n",
    "        self.skip = None\n",
    "        if in_ch != out_ch or up or down:\n",
    "            self.skip = get_conv_layer(in_ch, out_ch, 1, up=up, down=down)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        x:   [B, C_in, H, W]\n",
    "        emb: [B, emb_ch]\n",
    "        \"\"\"\n",
    "        orig = x\n",
    "        x = self.conv1(self.act(self.norm1(x)))\n",
    "\n",
    "        # Broadcast embedding and add\n",
    "        e = self.emb_layer(emb).unsqueeze(-1).unsqueeze(-1)  # [B, out_ch, 1, 1]\n",
    "        x = x + e\n",
    "\n",
    "        x = self.conv2(self.dropout(self.act(self.norm2(x))))\n",
    "    \n",
    "        x = x + (self.skip(orig) if self.skip is not None else orig)\n",
    "\n",
    "        return x * self.skip_scale\n",
    "\n",
    "# -----------------------------\n",
    "# Small EDM-style UNet\n",
    "# -----------------------------\n",
    "\n",
    "class EDMUNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    EDM-style UNet, scaled down to be < 10M params for typical choices.\n",
    "\n",
    "    - Conditioning: concatenates obs at input (x and obs same spatial size).\n",
    "    - Embedding: sigma + label combined and fed into all ResBlocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels: int,      # number of channels of x\n",
    "        obs_channels: int = 0,      # number of channels of obs\n",
    "        label_dim: int = 0,     # dimension of labels (0 = no labels)\n",
    "        base_channels: int = 64,\n",
    "        channel_mults=(1, 2, 2),   # e.g. [1,2,2] or [1,2,2,2]\n",
    "        num_res_blocks: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "        sigma_emb_dim: int = 64,\n",
    "        emb_dim: int = 256,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.img_channels = img_channels\n",
    "        self.obs_channels = obs_channels\n",
    "        in_channels = img_channels + obs_channels\n",
    "\n",
    "        # --- time/noise embedding ---\n",
    "        self.sigma_embed = PositionalEmbedding(sigma_emb_dim)\n",
    "        init_weights(self.sigma_embed)\n",
    "\n",
    "        self.time_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(sigma_emb_dim, emb_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "        for m in self.time_mlp:\n",
    "            init_weights(m)\n",
    "\n",
    "        # --- label embedding (optional) ---\n",
    "        if label_dim > 0:\n",
    "            self.label_embed = torch.nn.Linear(label_dim, emb_dim)\n",
    "            init_weights(self.label_embed)\n",
    "        else:\n",
    "            self.label_embed = None\n",
    "\n",
    "        # ------------- Encoder -------------\n",
    "        self.enc = torch.nn.ModuleList()\n",
    "        ch = base_channels\n",
    "        \n",
    "        ch_list = []  # track feature dims for skip sizes\n",
    "        for i, mult in enumerate(channel_mults):\n",
    "            out_ch = base_channels * mult\n",
    "            if i == 0:\n",
    "                self.enc.append(get_conv_layer(in_channels, out_ch, 3))\n",
    "            else:\n",
    "                self.enc.append(ResBlock(ch, out_ch, emb_dim, down=True, dropout=dropout))\n",
    "            ch = out_ch\n",
    "            ch_list.append(ch)\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.enc.append(ResBlock(ch, out_ch, emb_dim, dropout=dropout))\n",
    "                ch = out_ch\n",
    "                ch_list.append(ch)\n",
    "\n",
    "\n",
    "        # ------------- Decoder -------------\n",
    "        self.dec = torch.nn.ModuleList()\n",
    "        # We'll iterate channel_mults in reverse\n",
    "        for i, mult in reversed(list(enumerate(channel_mults))):\n",
    "            #out_ch = base_channels * mult\n",
    "            if i == len(channel_mults) - 1:\n",
    "                self.dec.append(ResBlock(out_ch, out_ch, emb_dim, dropout=dropout))\n",
    "                self.dec.append(ResBlock(out_ch, out_ch, emb_dim, dropout=dropout))\n",
    "            else:\n",
    "                self.dec.append(ResBlock(out_ch, out_ch, emb_dim, up=True, dropout=dropout))\n",
    "            for _ in range(num_res_blocks + 1):  # +1 to consume skips properly\n",
    "                # cat with skip features from encoder\n",
    "                in_ch = out_ch + ch_list.pop()\n",
    "                out_ch = base_channels * mult\n",
    "                self.dec.append(ResBlock(in_ch, out_ch, emb_dim, dropout=dropout))\n",
    "        \n",
    "        self.final_block = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_channels=out_ch, num_groups=32 if (out_ch % 32) == 0 else out_ch),\n",
    "            get_conv_layer(out_ch, img_channels, 3, init_mode=\"zeros\")\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            total_params = sum(p.numel() for p in self.parameters())\n",
    "            print(f\"SmallEDMUNet params: {total_params/1e6:.2f}M\")\n",
    "\n",
    "    def forward(self, x, sigma, labels=None, obs=None):\n",
    "        \"\"\"\n",
    "        x:      [B, Cx, H, W]   (noisy state)\n",
    "        obs:    [B, Co, H, W]   (initial condition / BCs, optional)\n",
    "        sigma:  [B] or [B,1]    (noise level)\n",
    "        labels: [B, label_dim]  (optional)\n",
    "        \"\"\"\n",
    "        # concat conditioning at input\n",
    "        if obs is not None and self.obs_channels > 0:\n",
    "            assert obs.shape[1] == self.obs_channels, f\"Expected obs with {self.obs_channels} channels, got {obs.shape[1]}\"\n",
    "            x = torch.cat([x, obs], dim=1)\n",
    "\n",
    "        # build embedding\n",
    "        emb_sigma = self.sigma_embed(sigma)        # [B, sigma_emb_dim]\n",
    "        emb = self.time_mlp(emb_sigma)             # [B, emb_dim]\n",
    "        if self.label_embed is not None and labels is not None:\n",
    "            emb = emb + self.label_embed(labels)   # combine as in EDM/DhariwalUNet\n",
    "\n",
    "        skips = []\n",
    "        # Encoder\n",
    "        for i, block in enumerate(self.enc):\n",
    "            x = block(x, emb) if isinstance(block, ResBlock) else block(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, block in enumerate(self.dec):\n",
    "            if x.shape[1] != block.in_channels:\n",
    "                x = torch.cat([x, skips.pop()], dim=1)\n",
    "            x = block(x, emb)\n",
    "\n",
    "        print(len(skips))\n",
    "\n",
    "        # Final conv block\n",
    "        x = self.final_block(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479c2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = EDMUNet(\n",
    "    img_channels=2,\n",
    "    obs_channels=0,\n",
    "    label_dim=2,\n",
    "    base_channels=64,\n",
    "    channel_mults=(1, 2, 2),\n",
    "    num_res_blocks=2,\n",
    "    dropout=0.0,\n",
    "    sigma_emb_dim=64,\n",
    "    emb_dim=256,\n",
    "    debug=False,\n",
    ").to(device)\n",
    "\n",
    "test_X = torch.randn(16, 2, 64, 64).to(device)\n",
    "test_labels = torch.randn(16, 2).to(device)\n",
    "test_sigma = torch.randn(16).to(device)\n",
    "\n",
    "test_out = net(test_X, test_sigma, labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b03f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyndiffenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
